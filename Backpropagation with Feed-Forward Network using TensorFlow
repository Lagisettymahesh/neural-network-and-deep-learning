import tensorflow as tf
X = tf.constant([[0., 0.],
                 [0., 1.],
                 [1., 0.],
                 [1., 1.]], dtype=tf.float32)
y = tf.constant([[0.],
                 [1.],
                 [1.],
                 [0.]], dtype=tf.float32)
tf.random.set_seed(42)
W1 = tf.Variable(tf.random.normal((2, 3)))
b1 = tf.Variable(tf.zeros((1, 3)))
W2 = tf.Variable(tf.random.normal((3, 1)))
b2 = tf.Variable(tf.zeros((1, 1)))
lr = 0.1
epochs = 3000
loss_history = []
def sigmoid(z):
    return tf.nn.sigmoid(z)

# BEFORE BACKPROP (Only forward pass)
z1 = tf.matmul(X, W1) + b1
a1 = sigmoid(z1)
z2 = tf.matmul(a1, W2) + b2
y_pred_before = sigmoid(z2)
loss_before = tf.reduce_mean(tf.square(y - y_pred_before))
# TRAINING (Backpropagation)
for _ in range(epochs):
    with tf.GradientTape() as tape:
        z1 = tf.matmul(X, W1) + b1
        a1 = sigmoid(z1)
        z2 = tf.matmul(a1, W2) + b2
        y_hat = sigmoid(z2)
        loss = tf.reduce_mean(tf.square(y - y_hat))
    grads = tape.gradient(loss, [W1, b1, W2, b2])

loss_after = tf.reduce_mean(tf.square(y - y_pred_after))

plt.plot(loss_history)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Reduction Using Backpropagation (TensorFlow)")
plt.show()
